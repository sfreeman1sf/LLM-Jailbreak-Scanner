# LLM-Jailbreak-Scanner
Detects prompt injection attacks in LLMs. Used in Outlier AI testing."Repo 2: Zero-Trust-MFA-Simulator
